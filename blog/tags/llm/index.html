<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.6">
<link rel="alternate" type="application/rss+xml" href="/vrndweb/blog/blog/rss.xml" title="Virtus ArGe Yazılım A.Ş. Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/vrndweb/blog/blog/atom.xml" title="Virtus ArGe Yazılım A.Ş. Blog Atom Feed"><title data-react-helmet="true">One post tagged with &quot;LLM&quot; | Virtus ArGe Yazılım A.Ş.</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;LLM&quot; | Virtus ArGe Yazılım A.Ş."><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/vrndweb/blog/blog/tags/llm"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/vrndweb/blog/img/virtusrndlogo.png"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/vrndweb/blog/blog/tags/llm"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/vrndweb/blog/blog/tags/llm" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/vrndweb/blog/blog/tags/llm" hreflang="x-default"><link rel="stylesheet" href="/vrndweb/blog/assets/css/styles.ba65bd99.css">
<link rel="preload" href="/vrndweb/blog/assets/js/runtime~main.3eb3703b.js" as="script">
<link rel="preload" href="/vrndweb/blog/assets/js/main.ca0fee71.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><a class="navbar__brand" href="/vrndweb/blog/"></a></div><div class="navbar__items navbar__items--right"></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/vrndweb/blog/blog/llm">LLM</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/vrndweb/blog/blog/microservice">Microservice</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;LLM&quot;</h1><a href="/vrndweb/blog/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/vrndweb/blog/blog/llm">LLM</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2025-06-19T13:29:08.000Z" itemprop="datePublished">June 19, 2025</time> · 5 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/hasanaliozkan/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://media.licdn.com/dms/image/v2/D4D03AQFCyAorQPfaFw/profile-displayphoto-shrink_400_400/B4DZaaczHHHEAk-/0/1746347963464?e=1755734400&amp;v=beta&amp;t=1CgRNWoVk0yKSIbJGhOvqcrhejSHOtJDoiqRsKwVUYs" alt="Hasan Ali Özkan"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/hasanaliozkan/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hasan Ali Özkan</span></a></div><small class="avatar__subtitle" itemprop="description">Buyuk Patron</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="table-of-contents"></a>Table of Contents<a class="hash-link" href="#table-of-contents" title="Direct link to heading">#</a></h2><ul><li><a href="#introduction">Introduction</a></li><li><a href="#1-what-is-a-large-language-model-llm">1. What is a Large Language Model (LLM)?</a></li><li><a href="#2-architecture-of-llms">2. Architecture of LLMs</a><ul><li><a href="#21-transformer-architecture">2.1 Transformer Architecture</a></li><li><a href="#22-pretraining-and-fine-tuning">2.2 Pretraining and Fine-tuning</a></li><li><a href="#23-tokenization">2.3 Tokenization</a></li></ul></li><li><a href="#3-training-large-language-models">3. Training Large Language Models</a><ul><li><a href="#31-dataset-collection">3.1 Dataset Collection</a></li><li><a href="#32-compute-requirements">3.2 Compute Requirements</a></li><li><a href="#33-optimization-algorithms">3.3 Optimization Algorithms</a></li></ul></li><li><a href="#4-deployment-strategies">4. Deployment Strategies</a><ul><li><a href="#41-inference-optimization">4.1 Inference Optimization</a></li><li><a href="#42-model-quantization">4.2 Model Quantization</a></li><li><a href="#43-serving-with-apis">4.3 Serving with APIs</a></li></ul></li><li><a href="#5-use-cases-of-llms">5. Use Cases of LLMs</a><ul><li><a href="#51-text-generation">5.1 Text Generation</a></li><li><a href="#52-code-generation">5.2 Code Generation</a></li><li><a href="#53-conversational-agents">5.3 Conversational Agents</a></li><li><a href="#54-search-and-retrieval">5.4 Search and Retrieval</a></li><li><a href="#55-document-summarization-and-translation">5.5 Document Summarization and Translation</a></li></ul></li><li><a href="#6-challenges-and-limitations">6. Challenges and Limitations</a><ul><li><a href="#61-hallucination">6.1 Hallucination</a></li><li><a href="#62-bias-and-fairness">6.2 Bias and Fairness</a></li><li><a href="#63-context-window-limitations">6.3 Context Window Limitations</a></li></ul></li><li><a href="#7-safety-and-ethics">7. Safety and Ethics</a><ul><li><a href="#71-misinformation-and-abuse">7.1 Misinformation and Abuse</a></li><li><a href="#72-data-privacy">7.2 Data Privacy</a></li><li><a href="#73-open-vs-closed-models">7.3 Open vs Closed Models</a></li></ul></li><li><a href="#8-popular-llms-and-frameworks">8. Popular LLMs and Frameworks</a></li><li><a href="#9-best-practices-for-using-llms">9. Best Practices for Using LLMs</a></li><li><a href="#10-future-directions">10. Future Directions</a></li><li><a href="#conclusion">Conclusion</a></li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h2><p>Large Language Models (LLMs) are AI systems trained to understand and generate human-like text. Built on neural network architectures like transformers, LLMs power applications such as chatbots, virtual assistants, and code assistants. Their size and complexity enable deep understanding of language, but also bring significant technical and ethical challenges.</p><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="1-what-is-a-large-language-model-llm"></a>1. What is a Large Language Model (LLM)?<a class="hash-link" href="#1-what-is-a-large-language-model-llm" title="Direct link to heading">#</a></h2><p>An LLM is a machine learning model, typically based on the <strong>transformer architecture</strong>, trained on vast amounts of text data to perform a wide variety of language tasks.</p><p>Key traits:</p><ul><li>Billion to trillion parameter scale</li><li>Trained on diverse corpora (web, books, code, social media)</li><li>Capable of few-shot and zero-shot learning</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="2-architecture-of-llms"></a>2. Architecture of LLMs<a class="hash-link" href="#2-architecture-of-llms" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="21-transformer-architecture"></a>2.1 Transformer Architecture<a class="hash-link" href="#21-transformer-architecture" title="Direct link to heading">#</a></h3><ul><li>Introduced by Vaswani et al. in 2017 (&quot;Attention is All You Need&quot;)</li><li>Core component: <strong>Self-Attention Mechanism</strong></li><li>Consists of encoder and decoder blocks (most LLMs use only decoders, like GPT)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="22-pretraining-and-fine-tuning"></a>2.2 Pretraining and Fine-tuning<a class="hash-link" href="#22-pretraining-and-fine-tuning" title="Direct link to heading">#</a></h3><ul><li><strong>Pretraining:</strong> Learn language patterns by predicting the next word (causal) or masked tokens (masked LM).</li><li><strong>Fine-tuning:</strong> Adapt model to specific tasks (e.g., Q&amp;A, summarization).</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="23-tokenization"></a>2.3 Tokenization<a class="hash-link" href="#23-tokenization" title="Direct link to heading">#</a></h3><ul><li>Text is split into tokens using algorithms like <strong>Byte-Pair Encoding (BPE)</strong> or <strong>SentencePiece</strong>.</li><li>Vocabulary size typically ranges from 32k to 100k tokens.</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="3-training-large-language-models"></a>3. Training Large Language Models<a class="hash-link" href="#3-training-large-language-models" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="31-dataset-collection"></a>3.1 Dataset Collection<a class="hash-link" href="#31-dataset-collection" title="Direct link to heading">#</a></h3><ul><li>Common sources: Common Crawl, Wikipedia, BooksCorpus, GitHub, Reddit</li><li>Needs deduplication and filtering to reduce noise and bias</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="32-compute-requirements"></a>3.2 Compute Requirements<a class="hash-link" href="#32-compute-requirements" title="Direct link to heading">#</a></h3><ul><li>Training large models requires <strong>massive GPU clusters</strong> (e.g., A100s, H100s)</li><li>Training GPT-3 required several thousand petaflop/s-days</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="33-optimization-algorithms"></a>3.3 Optimization Algorithms<a class="hash-link" href="#33-optimization-algorithms" title="Direct link to heading">#</a></h3><ul><li>Optimizers: <strong>AdamW</strong>, <strong>LAMB</strong></li><li>Techniques: gradient checkpointing, mixed-precision (FP16/BF16), ZeRO (Zero Redundancy Optimizer)</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="4-deployment-strategies"></a>4. Deployment Strategies<a class="hash-link" href="#4-deployment-strategies" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="41-inference-optimization"></a>4.1 Inference Optimization<a class="hash-link" href="#41-inference-optimization" title="Direct link to heading">#</a></h3><ul><li>Use libraries like <strong>ONNX Runtime</strong>, <strong>TensorRT</strong>, <strong>DeepSpeed Inference</strong></li><li>Scale across GPUs and nodes using <strong>model parallelism</strong> or <strong>tensor parallelism</strong></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="42-model-quantization"></a>4.2 Model Quantization<a class="hash-link" href="#42-model-quantization" title="Direct link to heading">#</a></h3><ul><li>Reduces model size and speeds up inference (e.g., 8-bit or 4-bit weights)</li><li>Frameworks: <strong>bitsandbytes</strong>, <strong>ggml</strong>, <strong>Optimum</strong></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="43-serving-with-apis"></a>4.3 Serving with APIs<a class="hash-link" href="#43-serving-with-apis" title="Direct link to heading">#</a></h3><ul><li>Expose models via REST or gRPC APIs using tools like <strong>FastAPI</strong>, <strong>Triton Inference Server</strong>, <strong>vLLM</strong></li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="5-use-cases-of-llms"></a>5. Use Cases of LLMs<a class="hash-link" href="#5-use-cases-of-llms" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="51-text-generation"></a>5.1 Text Generation<a class="hash-link" href="#51-text-generation" title="Direct link to heading">#</a></h3><ul><li>Creative writing, story generation, social media posts</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="52-code-generation"></a>5.2 Code Generation<a class="hash-link" href="#52-code-generation" title="Direct link to heading">#</a></h3><ul><li>Tools like GitHub Copilot, CodeWhisperer, ChatGPT Code Interpreter</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="53-conversational-agents"></a>5.3 Conversational Agents<a class="hash-link" href="#53-conversational-agents" title="Direct link to heading">#</a></h3><ul><li>Customer service bots, tutoring systems, virtual companions</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="54-search-and-retrieval"></a>5.4 Search and Retrieval<a class="hash-link" href="#54-search-and-retrieval" title="Direct link to heading">#</a></h3><ul><li>Embedding-based search, RAG (Retrieval-Augmented Generation)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="55-document-summarization-and-translation"></a>5.5 Document Summarization and Translation<a class="hash-link" href="#55-document-summarization-and-translation" title="Direct link to heading">#</a></h3><ul><li>Abstract and extractive summaries, multi-language support</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="6-challenges-and-limitations"></a>6. Challenges and Limitations<a class="hash-link" href="#6-challenges-and-limitations" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="61-hallucination"></a>6.1 Hallucination<a class="hash-link" href="#61-hallucination" title="Direct link to heading">#</a></h3><ul><li>Models may generate factually incorrect but plausible-sounding content</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="62-bias-and-fairness"></a>6.2 Bias and Fairness<a class="hash-link" href="#62-bias-and-fairness" title="Direct link to heading">#</a></h3><ul><li>LLMs inherit and sometimes amplify societal biases from their training data</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="63-context-window-limitations"></a>6.3 Context Window Limitations<a class="hash-link" href="#63-context-window-limitations" title="Direct link to heading">#</a></h3><ul><li>Limited to a certain number of tokens per prompt (e.g., 4k, 8k, 128k)</li><li>Long-context models (e.g., Claude, Gemini, GPT-4-Turbo) improve this</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="7-safety-and-ethics"></a>7. Safety and Ethics<a class="hash-link" href="#7-safety-and-ethics" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="71-misinformation-and-abuse"></a>7.1 Misinformation and Abuse<a class="hash-link" href="#71-misinformation-and-abuse" title="Direct link to heading">#</a></h3><ul><li>Risk of generating toxic, harmful, or deceptive outputs</li><li>Needs prompt moderation, safety classifiers, or RLHF (Reinforcement Learning from Human Feedback)</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="72-data-privacy"></a>7.2 Data Privacy<a class="hash-link" href="#72-data-privacy" title="Direct link to heading">#</a></h3><ul><li>Training on public data may expose personal or proprietary information</li><li>Techniques like <strong>differential privacy</strong> are under exploration</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor anchor__h3 anchorWithStickyNavbar_y2LR" id="73-open-vs-closed-models"></a>7.3 Open vs Closed Models<a class="hash-link" href="#73-open-vs-closed-models" title="Direct link to heading">#</a></h3><ul><li>Open-source (e.g., LLaMA, Mistral, Falcon): Transparent and modifiable</li><li>Closed-source (e.g., GPT-4, Gemini): Higher quality but less auditable</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="8-popular-llms-and-frameworks"></a>8. Popular LLMs and Frameworks<a class="hash-link" href="#8-popular-llms-and-frameworks" title="Direct link to heading">#</a></h2><table><thead><tr><th>Model</th><th>Developer</th><th>Size</th><th>Notes</th></tr></thead><tbody><tr><td>GPT-4</td><td>OpenAI</td><td>Undisclosed</td><td>Strong performance, closed</td></tr><tr><td>Claude 3</td><td>Anthropic</td><td>Undisclosed</td><td>Long context, safe RLHF</td></tr><tr><td>LLaMA 3</td><td>Meta</td><td>8B, 70B</td><td>Open weights</td></tr><tr><td>Gemini</td><td>Google DeepMind</td><td>Various</td><td>Integrated with Google tools</td></tr><tr><td>Mistral</td><td>Mistral AI</td><td>7B, Mixtral</td><td>Efficient, open source</td></tr><tr><td>Falcon</td><td>TII</td><td>7B, 40B</td><td>Strong open models</td></tr></tbody></table><p>Frameworks:</p><ul><li><strong>Transformers (Hugging Face)</strong></li><li><strong>LangChain</strong></li><li><strong>OpenLLM</strong></li><li><strong>vLLM</strong></li><li><strong>llama.cpp</strong></li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="9-best-practices-for-using-llms"></a>9. Best Practices for Using LLMs<a class="hash-link" href="#9-best-practices-for-using-llms" title="Direct link to heading">#</a></h2><ul><li>Use <strong>RAG</strong> to ground answers in private documents</li><li>Apply <strong>prompt engineering</strong> techniques for control</li><li>Monitor output for <strong>toxicity, bias, hallucination</strong></li><li>Fine-tune on task-specific data for higher accuracy</li><li>Always include <strong>human-in-the-loop</strong> for critical tasks</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="10-future-directions"></a>10. Future Directions<a class="hash-link" href="#10-future-directions" title="Direct link to heading">#</a></h2><ul><li><strong>Multimodal LLMs:</strong> Combining text, image, audio, and video</li><li><strong>Agent Architectures:</strong> Goal-driven AI that plans and executes</li><li><strong>Edge LLMs:</strong> Running models on low-resource devices</li><li><strong>Personalized AI:</strong> Adapting to individual preferences and memory</li></ul><hr><h2><a aria-hidden="true" tabindex="-1" class="anchor anchor__h2 anchorWithStickyNavbar_y2LR" id="conclusion"></a>Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">#</a></h2><p>LLMs represent a leap forward in AI capabilities, transforming industries from software development to healthcare. However, they are not magical solutions—they require careful handling, ethical oversight, and continuous optimization to unlock their full potential.</p><p>As these models continue to evolve, developers and organizations must balance innovation with responsibility.</p><hr></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/vrndweb/blog/blog/tags/llm">LLM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/vrndweb/blog/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/vrndweb/blog/blog/tags/artificial-intelligence">Artificial Intelligence</a></li></ul></div><div class="col col--3 text--right"><a aria-label="Read more about LLM" href="/vrndweb/blog/blog/llm"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/vrndweb/blog/">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/vrndweb/blog/assets/js/runtime~main.3eb3703b.js"></script>
<script src="/vrndweb/blog/assets/js/main.ca0fee71.js"></script>
</body>
</html>